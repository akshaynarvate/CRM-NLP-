# -*- coding: utf-8 -*-
"""CRM_EDA(NLP).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1zFt-Eg2fRuR4wQQZLqqyY6kP3Skuhi4N

# **Importing the Data**
"""

# Importing necessary libraries
import pandas as pd
import numpy as np

# Reading the excel file as DataFrame
data = pd.read_excel("/content/1000 leads.xlsx")
data.head()

data.shape

data.info()

# Checking for the null values
data.isnull().sum()

# Checking for the duplicate values
data.duplicated().sum()

"""# **Exploratory Data Analysis**"""

# Importing the necessary libraries
import matplotlib.pyplot as plt
import seaborn as sns

data.columns

data["Status "].value_counts()

data["Status "].unique()

data["Status "].replace({'NOt Converted': 'Not Converted', 'Conveted': 'Converted', 'Converted ': 'Converted'}, inplace=True)

data["Status "].unique()

data["Status "].value_counts()

data.rename(columns={"Status " : "Status"}, inplace=True)

data.head()

"""## **Balancing the imbalanced data**"""

# Lead name
len(data["Lead Name"].unique())

data[data["Lead Name"].duplicated()==True]["Lead Name"].unique()

plt.figure(figsize=(16, 6), dpi=90)
sns.countplot(data=data, x="Location")
plt.xticks(Rotation="vertical")
plt.show()

data["Location"].unique()

data["Location"].isnull().sum()

"""## **Fixing the null values in location with mode**"""

data["Location"].mode()[0]

data.loc[data["Location"].isnull(), "Location"] = data["Location"].mode()[0]

# Lower casing the Location column
data["Location"] = data["Location"].apply(lambda x: x.lower())
data["Location"].unique()

# Fetching country function
def country(x):
    if x == "australia":
        return "australia"
    elif x == "usa":
        return "usa"
    elif x == "uae":
        return "uae"
    else:
        return "india"

data["Country"] = data["Location"].apply(country)

sns.countplot(data=data, x="Country")
plt.show()

# Finding out which locations have more num of converted/non converted rate

plt.figure(figsize=(16,6), dpi=90)
sns.countplot(data=data, x="Location", hue="Status")
plt.xticks(Rotation="vertical")
plt.legend(loc="upper right")
plt.show()

data.head()

data["Status information"].isnull().sum()

# We will drop the row which have null textual data
data.dropna(inplace=True)

data.isnull().sum()

data["num_char"] = data["Status information"].apply(lambda x: len(x))

plt.figure(figsize=(16, 8), dpi=90)
sns.displot(data=data, x="num_char", hue="Status", aspect=1.2)
plt.show()

# Lets find the num of words per conversation
import nltk

nltk.download("punkt")

for i in data["Status information"][:10]:
    print(i)

import string
pun_word = string.punctuation

nltk.download("stopwords")
from nltk.corpus import stopwords

stop_words = stopwords.words("english")

# Custom stopwords where you will have negative words as well
stop_words2 = [i for i in stop_words if i not in ["not","no", "nor","don't", "aren't",
 "couldn't",
 "didn't",
 "doesn't",
 "hadn't",
 "hasn't",
 "haven't",
 "isn't",
 "mightn't",
 "mustn't",
 "needn't",
 "shouldn't",
 "wasn't",
 "weren't",
 "won't",
 "wouldn't" ]]

data["num_words"] = data["Status information"].apply(lambda x : len([i for i in nltk.word_tokenize(x) if i not in pun_word]))

# how many words present in ppl go not converting
plt.figure(figsize=(16, 6), dpi=90)
sns.displot(data=data, x="num_words", hue="Status")
plt.show()

for i in data["Status information"][-10:-1]:
    print([x.lower() for x in nltk.word_tokenize(i) if x not in pun_word and x not in stop_words2])

"""# **Spell** **Checking**

# **TextBlob**
"""

# Understanding the spell checker
from textblob import TextBlob

for i in data["Status information"][-10:-1]:
    txt = " ".join([x.lower() for x in nltk.word_tokenize(i) if x not in pun_word and x not in stop_words2])
    spell_check = TextBlob(txt)
    txt = spell_check.correct()
    print(txt)

"""## TextBlob doesn't do very good job in correcting chat spellings"""

# pyspellchecker
!pip install pyspellchecker

from spellchecker import SpellChecker
spellchecker = SpellChecker()

misspelling = spellchecker.unknown(["rnr", "mrng", "cal", "inst", "instrusted", "wid", "tim", "cal", "traning"])

for i in misspelling:
    print(spellchecker.correction(i))
    print(spellchecker.candidates(i))

"""# **Text Preprocessing**"""

import re

data.head()

df = data.copy()

# Label encoding the Locations
from sklearn.preprocessing import LabelEncoder
label_encoded = LabelEncoder()

df["Location"] = label_encoded.fit_transform(df["Location"])
df["Location"].head()

df["Status"].replace({"Not Converted": 0, "Converted": 1}, inplace=True)
df["Status"].head()

# Converted = 1 & Not Converted = 0
df.head()

nltk.download("wordnet")

nltk.download("omw-1.4")

# Lemmatization
from nltk.stem import WordNetLemmatizer
lemma = WordNetLemmatizer()
def preprocess2(txt):
    x = txt.lower()
    x = re.sub("\d+[/?]\w+[/?]\w+:|\d+[|]\w+[|]\w+:|\d+[/]\w+[/]\w+[(]\w+[)]:?", "", x)
    x = re.sub("int[a-z]+d$", "interested", x)
    x = re.sub("[\d+-?,'.]", "", x)
    x = [i for i in nltk.word_tokenize(x) if i not in stop_words2 and len(i)>1 and i not in pun_word] # word_tokenization, stop_word/punctuation removal]
    x = [lemma.lemmatize(i) for i in x] # there are still lot of incorrect spellings
    return " ".join(x)

# Stemming  
from nltk import PorterStemmer
porter_stem = PorterStemmer()
def preprocess(txt):
    x = txt.lower()
    x = re.sub("\d+[/?]\w+[/?]\w+:|\d+[|]\w+[|]\w+:|\d+[/]\w+[/]\w+[(]\w+[)]:?", "", x)
    x = re.sub("int[a-z]+d$", "interested", x)
    x = re.sub("[\d+-?,'.]", "", x)
    x = [i for i in nltk.word_tokenize(x) if i not in stop_words2 and len(i)>1 and i not in pun_word] # word_tokenization, stop_word/punctuation removal
    x = [lemma.lemmatize(i) for i in x] # there are still a lot of incorrect spelling
    return " ".join(x)

for i in data["Status information"][:12]:
    print(i)

for i in data["Status information"][:12]:
    print(preprocess2(i))

for i in data["Status information"][:12]:
    print(preprocess(i))

for i in data["Status information"][-10:-1]:
    print(i)

for i in data["Status information"][-10:-1]:
    print(preprocess2(i))

for i in data["Status information"][-10:-1]:
    print(preprocess(i))

df["Cleaned_txt"] = df["Status information"].apply(preprocess2)

"""## **We have cleaned the text, lets correct few specific spellings**"""

def check(x):
    txt = re.findall("int[a-z]+d$", x)
    if len(txt) != 0:
        return txt[0]
    else:
        return "None"

df["Cleaned_txt"].apply(check).value_counts()

spell = ["interested","intrstd","intrsd","intrtstd","intrrstd"]
for i in spell:
  print(porter_stem.stem(i))

# we fixed all interested words to its correct spellings becoz we wanted to have topics
df["Cleaned_txt"] = df["Cleaned_txt"].apply(lambda x: re.sub("int[a-z]+d$", "interested", x))

df.head()

for i in df["Cleaned_txt"][30:40]:
    print(i)

df["Status"].value_counts()

"""# Dealing with Imbalanced data"""

# Upsampling using resample class
from sklearn.utils import resample

# Separating the class
df_major = df[df["Status"] == 0]
df_minor = df[df["Status"] == 1]

df_minor_upsampled = resample(df_minor,
                              replace=True,    # Sample with replacement
                              n_samples= 700,  # To mathc majority class
                              random_state=20) # Reporducible 

# Combine majority class with upsampled minority class
df_upsampled = pd.concat([df_major, df_minor_upsampled], axis=0)

df_upsampled["Status"].value_counts()

"""# **WordCloud**"""

# Function to get all words
def word_freq(x):

    # Words get stored
    lst = []

    for word in x.split():
        lst.extend(word) # Adding all words to the list

    # Making a pandas series so we can apply value_counts()
    words = pd.Series(lst)
    return words

# [Not Converted = 0] Word Frequency
words_freq_not = word_freq(df[df["Status"]==0]["Cleaned_txt"].str)

top_not_converted_words = words_freq_not.value_counts().reset_index()[:30]

top_not_converted_words.head()

# [Converted = 1] Word frequency
words_freq_convert = word_freq(df[df["Status"]==1]["Cleaned_txt"].str)
top_converted_words = words_freq_convert.value_counts()[:30]
top_converted_words.head()

# Plotting the Word Frequency of [Not Converted = 0]
plt.figure(figsize=(12, 6), dpi=90)
sns.barplot(data=top_not_converted_words, x="index", y=0)
plt.xticks(rotation="vertical")
plt.show()

# Importing the WordCloud
from wordcloud import WordCloud

# Lets create the wordcloud 
wordcloud = WordCloud(width=800, height=800, background_color="white")
not_converted_wordcloud = wordcloud.generate(df[df["Status"]==0]["Cleaned_txt"].str.cat(sep=" "))

# Plot the WordCloud image
plt.figure(figsize=(8,8), facecolor=None)
plt.imshow(not_converted_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

converted_wordcloud = wordcloud.generate(df[df["Status"]==1]["Cleaned_txt"].str.cat(sep=" "))

# Plot the WordCloud image
plt.figure(figsize=(8,8), facecolor=None)
plt.imshow(converted_wordcloud)
plt.axis("off")
plt.tight_layout(pad=0)
plt.show()

"""# **Topic Modeling**

---


"""

# Using TF-IDF 
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
term_frequence = TfidfVectorizer(max_features=700) # We will be using TF for classification
count_vectorizer = CountVectorizer(max_features=500, ngram_range=(2, 2)) # we will use count_vectorizer for topic modelling

vector_txt2 = count_vectorizer.fit_transform(df_upsampled["Cleaned_txt"])

vector_txt = term_frequence.fit_transform(df_upsampled["Cleaned_txt"])

# Topic modeling using LDA(Latent Dirichlet Allocation)
from sklearn.decomposition import LatentDirichletAllocation

lda_model = LatentDirichletAllocation(n_components=2, learning_method='online',random_state=42,max_iter=5)

lda_top = lda_model.fit_transform(vector_txt2)

print("Document 0: ")
for i, topic in enumerate(lda_top[0]):
    print("Topic ", i, ": ", topic*100, "%")

# Let us check what are the top words that comprise the topics.
# This would give us a view what defines each of these topics
topics = dict()
vocab = count_vectorizer.get_feature_names()
for i, comp in enumerate(lda_model.components_):
    vocab_comp = zip(vocab, comp)
    sorted_words = sorted(vocab_comp, key=lambda x:x[1], reverse=True)[:9]
    topic = "Topic" + str(i) + ": "
    # print(topic)
    topics[topic] = []
    for t in sorted_words:
        topics[topic].append(t[0])
        # print(t[0], end=" ")
        # print("\n")

# Now we can understand what topics will have what words that are being considered
for k, v in topics.items():
    print(f"{k} : {v}")

df["Status information"][0]

df["Status"][0]

# Lets check for document 1
print("Document 1: ")
for i, topic in enumerate(lda_top[1]):
    print("Topic ", i, ": ", topic*100, "%")

# Lets check for document 2
print("Document : ")
for i, topic in enumerate(lda_top[2]):
    print("Topic ", i, ": ", topic*100, "%")

print(df["Status information"][2])
print(df["Status"][2])

# Lets check for document 3
print("Document 3: ")
for i, topic in enumerate(lda_top[3]):
    print("Topic ", i, ": ", topic*100, "%")

print(df["Status information"][3])
print(df["Status"][3])

# lets check for document 4
print("Document 4: ")
for i,topic in enumerate(lda_top[4]):
  print("Topic ",i,": ",topic*100,"%")

print(df["Status information"][4])
print(df["Status"][4])

# Topic modeling system
def topic_model():
    user_mgs = input("Enter your mgs: ")
    x = preprocess2(user_mgs)
    x = count_vectorizer.transform([x])
    lda_x = lda_model.transform(x)
    tpics = []
    tpic = lambda x: "not interested" if x == 0 else "interested"
    for i,topic in enumerate(lda_x[0]):
        print("Topic ", i, ": ", topic*100, "%")
        tpic_name = tpic(i)
        percent = topic*100
        tpics.append([tpic_name, percent])
    return tpics

print(df["Status information"][700])
print(df["Status"][700])

print(df["Status information"][600])
print(df["Status"][600])

my_topic = topic_model()

for i in my_topic:
    print(i)

topic_model()

print(df_upsampled["Status information"][200])
print(df_upsampled["Status"][200])

"""# **Topic Modeling using Gensim(LDA)**"""

import gensim
from gensim import corpora

# Creating a clean corpus
corpus = [i.split() for i in df["Cleaned_txt"]]

corpus[:5]

# Creating a dictionary first
dict_ = corpora.Dictionary(corpus)

print(dict_)

# Converting list of documents (corpus) into Document Term Matrix using the dictionary 
doc_term_matrix = [dict_.doc2bow(i) for i in corpus]

doc_term_matrix

"""### **Implementing LDA now**"""

from gensim.models import LdaModel

Lda = LdaModel
ldamodel = Lda(doc_term_matrix, num_topics=2, id2word=dict_, passes=1, random_state=0, eval_every=None)

# Print the topics with the indexes: 0,1:

ldamodel.print_topics()

# we need to manually check whethere the topics are different from one another or not

# Extracting Topics
for i in ldamodel.print_topics(num_topics=2, num_words=10):
    print(i)

# Printing the topic associations with the documents
count = 0
for i in ldamodel[doc_term_matrix]:
    print("doc : ", count, i)
    count += 1

"""# **Classficiation Modeling**"""

df_upsampled["Status information"][5]

df_upsampled.head()

add_col = df_upsampled["Location"].values.reshape(-1, 1)

print(add_col)

vector_txt.toarray().shape

X = vector_txt.toarray()
y = df_upsampled["Status"]

# Train, test split
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=30)
for i in [X_train, X_test, y_train, y_test]:
    print(i.shape)

# Evaluation metrics
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score, precision_score

# Naive Bayes models
from sklearn.naive_bayes import GaussianNB, BernoulliNB, MultinomialNB

# Others
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression

# Creating a dictionary to keep records
results = dict()
results["Algorithm"] = []
results["Accuracy"] = []
results["Precision"] = []

for i in [GaussianNB(), BernoulliNB(), MultinomialNB()]:
    model = i.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    confus_matrix = confusion_matrix(y_test, y_pred)
    classficaton_report = classification_report(y_test, y_pred)
    precision = precision_score(y_test, y_pred)
    print(f"Algo: {i}")
    print(f"acc: {accuracy}\n")
    print("-"*55)
    print(f"{confus_matrix}\n")
    print("-"*55)
    print(f"{classification_report}\n")
    results["Algorithm"].append(i)
    results["Accuracy"].append(accuracy)
    results["Precision"].append(precision)

"""# **Ensembling technique**"""

for i in [ExtraTreesClassifier(), RandomForestClassifier(), AdaBoostClassifier(), GradientBoostingClassifier()]:
  model = i.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  con_m = confusion_matrix(y_test, y_pred)
  c_r = classification_report(y_test, y_pred)
  p_s = precision_score(y_test, y_pred)
  print(f"Algo: {i}")
  print(f"acc: {acc}\n")
  print("-"*55)
  print(f"{con_m}\n")
  print("-"*55)
  print(f"{c_r}\n")
  results["Algorithm"].append(i)
  results["Accuracy"].append(acc)
  results["Precision"].append(p_s)

for i in [SVC(), LogisticRegression(max_iter=1000)]:
  model = i.fit(X_train, y_train)
  y_pred = model.predict(X_test)
  acc = accuracy_score(y_test, y_pred)
  con_m = confusion_matrix(y_test, y_pred)
  c_r = classification_report(y_test, y_pred)
  p_s = precision_score(y_test, y_pred)
  print(f"Algo: {i}")
  print(f"acc: {acc}\n")
  print("-"*55)
  print(f"{con_m}\n")
  print("-"*55)
  print(f"{c_r}\n")
  results["Algorithm"].append(i)
  results["Accuracy"].append(acc)
  results["Precision"].append(p_s)

# Without Location involved
pd.DataFrame(results)

"""SO we came to understand that TF-IDF works well for classification of customer wherther the'll be converted or not

and Count vectorizer is working great for topic modeling

So i will consider both for my deployment(POC)

# Classification system
"""

model = ExtraTreesClassifier()

model.fit(X_train, y_train)

y_pred = model.predict(X_test)
acc = accuracy_score(y_test, y_pred)
con_m = confusion_matrix(y_test, y_pred)
c_r = classification_report(y_test, y_pred)
p_s = precision_score(y_test, y_pred)
print(f"Algo: {model}")
print(f"acc: {acc}\n")
print("-"*55)
print(f"{con_m}\n")
print("-"*55)
print(f"{c_r}\n")

model2 = SVC()
model2.fit(X_train, y_train)
y_pred = model2.predict(X_test)
acc = accuracy_score(y_test, y_pred)
con_m = confusion_matrix(y_test, y_pred)
c_r = classification_report(y_test, y_pred)
p_s = precision_score(y_test, y_pred)
print(f"Algo: {model2}")
print(f"acc: {acc}\n")
print("-"*55)
print(f"{con_m}\n")
print("-"*55)
print(f"{c_r}\n")

# Gaussian NB
model3 = GaussianNB()
model3.fit(X_train, y_train)
y_pred = model3.predict(X_test)
acc = accuracy_score(y_test, y_pred)
con_m = confusion_matrix(y_test, y_pred)
c_r = classification_report(y_test, y_pred)
p_s = precision_score(y_test, y_pred)
print(f"Algo: {model3}")
print(f"acc: {acc}\n")
print("-"*55)
print(f"{con_m}\n")
print("-"*55)
print(f"{c_r}\n")

# Multinomial
model4 = BernoulliNB()
model4.fit(X_train, y_train)
y_pred = model4.predict(X_test)
acc = accuracy_score(y_test, y_pred)
con_m = confusion_matrix(y_test, y_pred)
c_r = classification_report(y_test, y_pred)
p_s = precision_score(y_test, y_pred)
print(f"Algo: {model4}")
print(f"acc: {acc}\n")
print("-"*55)
print(f"{con_m}\n")
print("-"*55)
print(f"{c_r}\n")

# Classification system
def Status(user):
    x = user
    x = preprocess2(x)
    x = term_frequence.transform([x])
    x = model3.predict(x.toarray())
    if x == 0:
        return "Not Convertable"
    else:
        return "Convertable"

j = "3/3/12roahn hello miss jawsal 3/3/12 yes, needed service i can pay immediately"

mgs = input()
Status(mgs)

for i in [200, 221, 222, 608, 610, 567]:
    print(df.loc[i, "Status information"])
    print(df.loc[i, "Status"])

"""# Pickle file"""

import pickle

# Pickling the topic modeling files
pickle.dump(count_vectorizer, open("count_vectorizer.pkl", "wb"))
pickle.dump(lda_model, open("lda_model.pkl", "wb"))

pickle.dump(stop_words2, open("stopwords.pkl", "wb"))

# Pickling for classification POC
import pickle

pickle.dump(term_frequence, open("term_frequency_vectorizer.pkl", "wb"))
pickle.dump(model3, open("GNB.pkl", "wb"))

